{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hw1(tensor_tutorial).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM/9O2PUb5d3y22URYmGo6N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"TC7b4rC3XxUZ","executionInfo":{"status":"ok","timestamp":1633684843474,"user_tz":-540,"elapsed":26684,"user":{"displayName":"‍유욱현(학부생-소프트웨어전공)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01584191448701961186"}}},"source":["import torch\n","import numpy as np\n","\n","data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data) ## data to tensor\n","\n","np_array = np.array(data) ## data to array\n","x_np = torch.from_numpy(np_array) ## np.array to tensor\n","\n","x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n","print(f\"Random Tensor: \\n {x_rand} \\n\")\n","\n","shape = (2,3,) ## tensor의 shape 정의\n","rand_tensor = torch.rand(shape) ## shape을 통해 tensor 생성 가능\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n","\n","tensor = torch.rand(3,4)\n","\n","## 텐서의 속성은 텐서의 모양(shape), 자료형(datatype) 및 어느 장치에 저장되는지를 나타냄\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")\n","\n","# GPU가 존재하면 텐서를 이동합니다\n","if torch.cuda.is_available():\n","  tensor = tensor.to('cuda')\n","\n","tensor = torch.ones(4, 4)\n","tensor[:,1] = 0 ## 2열을 0으로 초기화\n","print(tensor)\n","\n","t1 = torch.cat([tensor, tensor, tensor], dim=1) ## tensor 연결\n","print(t1)\n","\n","# 요소별 곱(element-wise product)을 계산합니다\n","print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n","# 다른 문법:\n","print(f\"tensor * tensor \\n {tensor * tensor}\")\n","\n","print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\") ## 텐서끼리 행렬 곱\n","# 다른 문법:\n","print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")\n","\n","print(tensor, \"\\n\")\n","tensor.add_(5) ## 메모리는 절약되나 기록이 없어서 도함수 계산이 안됨.\n","print(tensor)\n","\n","t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")\n","\n","t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\") ## 텐서에 더해도 numpy에 적용됨\n","\n","n = np.ones(5)\n","t = torch.from_numpy(n) ## numpy to tensor\n","\n","np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ooymw3BMM-pK"},"source":[""],"execution_count":null,"outputs":[]}]}