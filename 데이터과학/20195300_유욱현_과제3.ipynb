{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20195300_유욱현_과제3.ipynb","provenance":[],"authorship_tag":"ABX9TyP6gdaII/ct55EWw+qwLn0Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gk-iMxKRn4P8","executionInfo":{"status":"ok","timestamp":1634385471681,"user_tz":-540,"elapsed":29270,"user":{"displayName":"‍유욱현(학부생-소프트웨어전공)","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01584191448701961186"}},"outputId":"c7ddafd5-2254-4266-dbf2-0ffec3fad50d"},"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","x_train = torch.FloatTensor([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n","y_train = torch.LongTensor([2,2,2,1,1,1,0,0])\n","# y_train = torch.FloatTensor([ [0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0] ])\n","\n"," \n","W = torch.zeros(4, 3, requires_grad=True)\n","b = torch.zeros(1, 3, requires_grad=True)\n","optimizer = torch.optim.Adam([W,b], lr=0.1)\n","\n","for epoch in range(3001):\n","  # hypothesis = torch.softmax(torch.mm(x_train, W)+b, dim=1)\n","  # cost = -torch.mean(torch.sum(y_train * torch.log(hypothesis), dim=1))\n","  # z = torch.mm(x_train, W)+b\n","  model = nn.Linear(4,3)  ## 간편하게 선형회귀 표현\n","  optimizer = torch.optim.Adam(model.parameters(),lr=1)\n","  z = model(x_train)\n","  cost = F.cross_entropy(z, y_train)\n","  hypothesis = (torch.mm(x_train, W)+b).softmax(dim=1)\n","  # cost = -(y_train * torch.log(hypothesis)).sum(dim=1).mean()\n","\n"," \n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n"," \n","  if epoch % 300 == 0:\n","    print(\"epoch: {}, cost: {:.6f}\".format(epoch, cost.item()))\n","\n","\n","## x 가 [1,11,10,9], [1,3,4,3], [1,1,0,1] 일 때, y값은? ##\n","W.requires_grad_(False)\n","b.requires_grad_(False)\n","x_test = torch.FloatTensor([[1,11,10,9], [1,3,4,3], [1,1,0,1]])\n","test_all = torch.softmax(torch.mm(x_test, W)+b, dim=1)\n","print(test_all)\n","print(torch.argmax(test_all, dim=1))\n","\n","\n","\n","\n","### Softmax Regression in Sklearn ###\n","## logistic regression, softmax regression are implemented \n","## if y_train has more than one kind of value, then softmax regression is executed.\n","\n"," \n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","x_train = np.array([ [1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5],\n","                     [1,2,5,6], [1,6,6,6], [1,7,7,7] ])\n","# y에 0, 1, 2 등 둘 이상의 class가 존재 => softmax regression \n","y_train = np.array([ 2, 2, 2, 1, 1, 1, 0, 0 ])\n","logistic = LogisticRegression() # 모델 생성 \n","logistic.fit(x_train, y_train) # 학습\n","pred = logistic.predict([[1,11,10,9], [1,3,4,3], [1,1,0,1]]) # test case (값 예측) \n","print(pred) # 출력"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 0, cost: 1.472518\n","epoch: 300, cost: 1.817991\n","epoch: 600, cost: 2.386678\n","epoch: 900, cost: 1.128857\n","epoch: 1200, cost: 1.713600\n","epoch: 1500, cost: 1.666236\n","epoch: 1800, cost: 1.961866\n","epoch: 2100, cost: 0.998841\n","epoch: 2400, cost: 1.700381\n","epoch: 2700, cost: 1.611539\n","epoch: 3000, cost: 1.590193\n","tensor([[0.3333, 0.3333, 0.3333],\n","        [0.3333, 0.3333, 0.3333],\n","        [0.3333, 0.3333, 0.3333]])\n","tensor([0, 0, 0])\n","[0 2 2]\n"]}]},{"cell_type":"code","metadata":{"id":"9xYnw_FLp8uD"},"source":[""],"execution_count":null,"outputs":[]}]}